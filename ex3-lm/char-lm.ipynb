{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with (unsmoothed character-level maximum likelihood) Language Models\n",
    "\n",
    "In this exercise we'll play with so called unsmoothed character-level maximum likelihood language models and discuss how impressively effective can even these simple models be for modeling language.\n",
    "\n",
    "There will be one singificant difference from what we've seen so far -- the language model will consider characters as its \"atomic units\", as opposed to words. This has its pros and cons, but we'll stick with characters due to the simplicity of the resulting model. You will find that we'll need surprisingly little amount of code for what we are trying to do.\n",
    "\n",
    "Speaking of which, what is it that we want to do exactly? Well, contrary to the model's name, the idea is actually very simple. We want a model that will receive $n$ characters as input and guess the next character in the sequence (hence the **character-level** part of the title). These $n$ characters the model sees are also known as \"history\" or the \"order\" of a language model.\n",
    "\n",
    "Looking at the task intuitively, we could easily rephrase it as \"looking at the history $h$, tell which character is likely to go next\". Humans have whole lifetimes of reading to train their internal language models for this task, and it turns out we are pretty good at that! In our case we'll try to learn this language model from English text, namely the works of Shakespeare.\n",
    "\n",
    "Mathematically speaking, we'll try to learn function $P(c|h)$ where $h$ is the history and $c$ is a character. Function $P$ then describes the likelihood of character $c$ following the history $h$. This should once again make sense intuitively, as $P(\\texttt{o}, \\texttt{hell})$ should be higher than say $P(\\texttt{l}, \\texttt{hell})$.\n",
    "\n",
    "So how should we go about implementing something like this? In a very straightforward manner: let's just count the number of times a character $c$ follows the history $h$ and divide it by the total number of letters that follow $h$ in the text we are provided (this is what's called **maximum likelihood** estimate). The reason why we call this Language Model **unsmoothed** is because when we do not see a letter following $h$, we simply set the probability to zero. That can potentially cause some trouble, but it should not be a big problem in our case.\n",
    "\n",
    "Let's put it together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two components in Python's standard library that can help us implement our language model -- a `defaultdict` defaulting to a [`Counter`](https://docs.python.org/3.7/library/collections.html#collections.Counter). That will then allow us to do things like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a sample Language Model\n",
    "sample_lm = defaultdict(Counter)\n",
    "\n",
    "# Add a count for character 'p' that follows the history 'hel'\n",
    "sample_lm['hel']['p'] += 1\n",
    "# Add a count for character 'l' that follows the history 'hel'\n",
    "sample_lm['hel']['l'] += 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this setup to easily get the number of characters we saw after a particular history (which should be quite helpful for implementing the normalization step) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sample_lm['hel'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with this knowledge, it should not be a problem to implement the following `train_char_lm` function that takes an input text and returns a trained language model of specific order.\n",
    "\n",
    "*Note: the language model should return probabilities. You can easily get them just by normalizing with the total number of characters seen after some history (previous cell). Creating a new dictionary for the output should be the easiest way of doing so.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTINEL = '~'\n",
    "def train_char_lm(text, order=3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (str): the input text to train the language model on\n",
    "        order (int): the order of the language model\n",
    "    \n",
    "    Returns:\n",
    "        dict[list]: a dictionary with a list of (char, probability)\n",
    "        tuples for each encountered history.\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        lm = train_char_lm('what am i doing here', order=3)\n",
    "        print(lm['wha'])\n",
    "        # [('t', 1.0)] \n",
    "        \n",
    "    \"\"\"\n",
    "    padding = SENTINEL * order\n",
    "    text = padding + text\n",
    "\n",
    "    lm = defaultdict(Counter)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, training a language model should be quite straightforward (and take only a few seconds on modern hardware):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = train_char_lm(open('./shakespeare.txt').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a couple of tests to ensure your language model got trained correctly ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(lm['hel']) == list\n",
    "assert type(lm['hel'][0]) == tuple\n",
    "assert type(lm['hel'][0][0]) == str\n",
    "assert type(lm['hel'][0][1]) == float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm['hel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm['DUK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented the `train_char_lm` function correctly, you should see above that `hel` is generally followed by `p`, `l` or `d` and that once we find `DUK` in this Shakespeare's text, we can be sure that it si followed by `E`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating from the model\n",
    "\n",
    "Once we have the model, generating from it is also quite easy. We'll just take the history, look at the last `order` characters, obtain a probability distribution and sample the next character based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def sample_char(lm, history, order):\n",
    "    h = history[-order:]\n",
    "    dist = lm[h]\n",
    "    \n",
    "    # A poor man's weighted sampling method\n",
    "    x = random()\n",
    "    for c, v in dist:\n",
    "        x = x - v\n",
    "        if x <= 0:\n",
    "            return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then want to generate say $k$ characters in a row, we'll just pass in the initial history and sample the next character $k$ times, while passing the newly generated characters back to the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, order, k=1000):\n",
    "    \n",
    "    history = SENTINEL * order\n",
    "    text = ''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done all this, we can finally experiment with trained models!\n",
    "\n",
    "#### Generating text from language models of various orders\n",
    "\n",
    "Let's start slow with a **bi-gram model (order ot 2)**. Note that this model will only look at the history of the past two characters when generating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 2\n",
    "lm = train_char_lm(open('./shakespeare.txt').read(), order=order)\n",
    "print(generate_text(lm, order=order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the results do not look all too good -- let's try to up the **order to 4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "lm = train_char_lm(open('./shakespeare.txt').read(), order=order)\n",
    "print(generate_text(lm, order=order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, interesting. Will it help if we go up? Let's bump it up even more to **order of 7**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 7\n",
    "lm = train_char_lm(open('./shakespeare.txt').read(), order=order)\n",
    "print(generate_text(lm, order=order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 10\n",
    "lm = train_char_lm(open('./shakespeare.txt').read(), order=order)\n",
    "print(generate_text(lm, order=order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with order of 2 and 4 we generally get gibberish, but once we move towards 7 to 10 (which in English basically corresponds to roughly 1.5 to 2 short words), text that would have passed as something a Shakespeare might have written! The ugly truth about the generated text is that quite a few parts of it are actually copied straight from the training text. It is kind of understandable -- we never let the model \"experiment\" with things that may not be in the training text, but you can try to fix that in the bonus part of the exercise.\n",
    "\n",
    "### Shakespearean autocomplete\n",
    "\n",
    "One of the interesting usecases of well-trained language models is something we all know from (smart)phones: autocomplete. Given the training text we have, it would be one heck of an overstatement to call any language model that sees nothing else \"well-trained\" but still, since we have a trained langauge model, let's use it to autocomplete sentences for us!\n",
    "\n",
    "Yes, in standard setup this task is usually done with word-level language models. But with a few changes, we can use character-level language models as well.\n",
    "\n",
    "Here is what we are going to do: we'll use the beginning of the text (or sentence) we want to autocomplete as a history and then sample as many characters as necessary, until we get a single word. In practice, this means we'll sample characters until we hit a word boundary (basically one of `[' ', '?', '!', '.', ';', ',', '\\n']`). Doing this multiple times will provide various auto completitions, just like an autocompletion engine.\n",
    "\n",
    "Your task is to implement this autocompletion process in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(lm, order, prompt, n=3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    \n",
    "        lm (dict[str, list]): A trained language model.\n",
    "        order (int): The order of the language model.\n",
    "        prompt (str): The prompt to base the completion on.\n",
    "        n (int): The number of completions.\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: a list of n possible completitions for\n",
    "        the provided prompt.\n",
    "    \"\"\"\n",
    "    completions = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you implement it correctly, the following cell should output `['think']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 3\n",
    "lm = train_char_lm('what do you think about it', order=order)\n",
    "autocomplete(lm, order, 'do you ', n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if that's the case, see if you can generate some more interesting, \"Shakespearean\" autocompletions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 7\n",
    "lm = train_char_lm(open('./shakespeare.txt').read(), order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'I would ',\n",
    "    'How do I ',\n",
    "    'I want a ',\n",
    "    'There are some '\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    print(p, ':', autocomplete(lm, order, p, n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final words\n",
    "\n",
    "As we saw, it does not take too much code to get together a language model that produces some \"readable\" text (for some definition of readable). All we need is to have some suitable training text and look a couple characters back at every point in it.\n",
    "\n",
    "If you look at the generated text a bit more closely though (especially for higher-order language models), you may notice that it copies quite a few words directly from the source text. Some could of course say that \"humans do that too\" and they would be obviously correct to some extend. And before you'd know, this discussion would end with questions like \"what does it actually mean to be *creative*?\" and we certainly do not want to go there. The larger point here is that text can be generated even with a simple model like this, although its \"inspiration\" may come directly from the data it saw in training. \n",
    "\n",
    "This should help put things into perspective when we see text generated by [various neural models](https://transformer.huggingface.co/doc/gpt2-large). They are indeed quite impressive, but not because they can generate text: even a simple character-level model can do that. What such a simple model cannot do, however, is to be aware of context. And it turns out the neural models are somehow not too bad at doing so.\n",
    "\n",
    "Hopefully, this quick exercise did make you at least a bit curious to check them out!\n",
    "\n",
    "*Note: the gist of this exercise is based on [this article by Yoav Goldberg](https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139). You may also appreciate [Peter Norvig's](https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb) article on playing with word-level language models.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Smoothed character-level language model\n",
    "\n",
    "As we've said above, the model we described is **unsmoothed**, which means that things that we do not see in the training dataset are considered not to exist (or to occur with zero probability). If we only sample text from known prompts (i.e. if we only use the history we are sure has occured in the training text) this does not cause too much trouble, but try some more \"exotic\" promts for the autocompletion above (like `'THERE ARE FOUR '`) and observe how the whole procedure fails on a `KeyError`.\n",
    "\n",
    "This situation can be fixed though. Compared to word-level language models, it is much easier to enumerate all possible options (it's basically just all the characters that we expect to find in the input text -- about 70 in total).\n",
    "\n",
    "In this bonus part, your task is to implement a [simple Laplace (add one) smoothing](https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf#page=48) in the training procedure and see whether this addition helps you generate better texts and autocompletions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
