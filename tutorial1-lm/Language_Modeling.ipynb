{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06.RNN-Language-Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4d8H4-jC7-CC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Language Modeling with Recurrent Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "uCEhFpZK7-CF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this tutorial we'll try out what the \"modern\" Language Modeling (LM) looks like, see what it takes to implement one using PyTorch and then use the trained model to generate some text.\n",
        "\n",
        "Here is a list of some relevant references:\n",
        "\n",
        "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture8.pdf\n",
        "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture9.pdf\n",
        "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "* https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "* https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model"
      ]
    },
    {
      "metadata": {
        "id": "tSm3-2cY3Xbk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As the first order of business, we import some useful Python libraries, along with [PyTorch](https://pytorch.org/).\n",
        "\n",
        "**Note**: this tutorial expects at least basic familiarity with PyTorch. We strongly encourage you to get familiar with the [\"blitz\" introduction](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html) into this framework. \n",
        "\n",
        "**Technical note**: this tutorial also expects PyTorch to be installed. To do so, we encourage you to follow the [\"Get started\"](https://pytorch.org/get-started/locally/) guide. Note that this is not necessary when playing with PyTorch within the Google Collaboratory environment."
      ]
    },
    {
      "metadata": {
        "id": "d_P1Z1No7-CL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter, OrderedDict\n",
        "from copy import deepcopy\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "random.seed(1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7dWeZNF4h62",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following cell allows us to use the speed-ups of GPU, if it is available."
      ]
    },
    {
      "metadata": {
        "id": "jLjSzax47-Cd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "gpus = [0]\n",
        "torch.cuda.set_device(gpus[0])\n",
        "\n",
        "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
        "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fTfO7e57-Cy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data loading and preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "44IVB4fP5M3E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to try out Language Modeling on a relevant data source, we'll use the [Penn Treebank](https://nlpprogress.com/english/language_modeling.html#penn-treebank) dataset, which is a very common benchmark for Language modeling.\n",
        "\n",
        "The dataset is comprised of various parts of WSJ articles, which have already been tokenized and split into sentences, one per each line of the input file. Furthermore, only the top 10,000 most frequent words were used -- all of the others have been replaced with the special `<unk>` word.\n",
        "\n",
        "Running the following cell should download the dataset, provided that `wget` is installed in your environment."
      ]
    },
    {
      "metadata": {
        "id": "dBMUymbj8ynN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/NaiveNeuron/nlp-exercises/master/tutorial1-lm/data/ptb.train.txt\n",
        "!wget https://raw.githubusercontent.com/NaiveNeuron/nlp-exercises/master/tutorial1-lm/data/ptb.test.txt\n",
        "!wget https://raw.githubusercontent.com/NaiveNeuron/nlp-exercises/master/tutorial1-lm/data/ptb.valid.txt  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RLx9eBDP83Qe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Although the structure described above makes this dataset nicely human-readable, it needs to be converted into a format that would be consumable by a Neural Network model, which will serve as a testbed for our experiments. To simplify this conversion, we introduce the following helper functions."
      ]
    },
    {
      "metadata": {
        "id": "BkyFzGuY4IAv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_index):\n",
        "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<unk>\"], seq))\n",
        "    return LongTensor(idxs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TVHWuzuJ7-C8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_ptb_dataset(filename, word2index=None):\n",
        "    corpus = open(filename, 'r', encoding='utf-8').readlines()\n",
        "    corpus = flatten([co.strip().split() + ['<eos>'] for co in corpus])\n",
        "    \n",
        "    if word2index == None:\n",
        "        vocab = list(set(corpus))\n",
        "        word2index = {'<unk>': 0}\n",
        "        for vo in vocab:\n",
        "            if word2index.get(vo) is None:\n",
        "                word2index[vo] = len(word2index)\n",
        "    \n",
        "    return prepare_sequence(corpus, word2index), word2index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BvxBEVrp7-DD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# borrowed code from https://github.com/pytorch/examples/tree/master/word_language_model\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).contiguous()\n",
        "    if USE_CUDA:\n",
        "        data = data.cuda()\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ohnc9GCY7-DV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch(data, seq_length):\n",
        "     for i in range(0, data.size(1) - seq_length, seq_length):\n",
        "        inputs = Variable(data[:, i: i + seq_length])\n",
        "        targets = Variable(data[:, (i + 1): (i + 1) + seq_length].contiguous())\n",
        "        yield (inputs, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8P6uufb8p9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With all the helper functions prepared, we are ready to convert the data from a sequence of words (strings) into a sequence of integers, where each integer represents a specific word.\n",
        "\n",
        "Note that we are only interested in the `word2index` dictionary when loading the training dataset -- in all the other datasets we use the previously created dictionary to encode the input data."
      ]
    },
    {
      "metadata": {
        "id": "XwKFlNhp7-Dh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data, word2index = prepare_ptb_dataset('./ptb.train.txt',)\n",
        "dev_data , _ = prepare_ptb_dataset('./ptb.valid.txt', word2index)\n",
        "test_data, _ = prepare_ptb_dataset('./ptb.test.txt', word2index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RKIMf669pAj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a sanity check, we can try to see how many items can be found in the `word2index` dictionary."
      ]
    },
    {
      "metadata": {
        "id": "yu9EZIeo7-Dv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(word2index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x4RFrnwL9mpl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we will see in a bit, being able to convert a word to its unique index is very useful. It turns out, the invese can be just as useful, so we'll create the `index2word` dictionary in the next cell."
      ]
    },
    {
      "metadata": {
        "id": "OVBA6EaW7-EF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index2word = {v:k for k, v in word2index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1CMYjawX7-EO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modeling "
      ]
    },
    {
      "metadata": {
        "id": "WxuN98O17-EX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"./images/rnnlm-architecture.png\">\n",
        "Image borrowed from http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture8.pdf"
      ]
    },
    {
      "metadata": {
        "id": "oFgyv6dn-Mct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following cell we'll define a very simple Language Model by extending the `nn.Module` PyTorch provides.\n",
        "\n",
        "Note that we allow this module to have a variable `embedding_size`, `hidden_size`,  number of layers (`n_layers`) as well as Dropout probability `dropout_p` on the embedding layer."
      ]
    },
    {
      "metadata": {
        "id": "Oo4P3H1U7-Ei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module): \n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, n_layers=1, dropout_p=0.5):\n",
        "\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        \n",
        "    def init_weight(self):\n",
        "        self.embed.weight = nn.init.xavier_uniform_(self.embed.weight)\n",
        "        self.linear.weight = nn.init.xavier_uniform_(self.linear.weight)\n",
        "        self.linear.bias.data.fill_(0)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
        "        context = Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))\n",
        "        return (hidden.cuda(), context.cuda()) if USE_CUDA else (hidden, context)\n",
        "    \n",
        "    def detach_hidden(self, hiddens):\n",
        "        return tuple([hidden.detach() for hidden in hiddens])\n",
        "    \n",
        "    def forward(self, inputs, hidden, is_training=False): \n",
        "        embeds = self.embed(inputs)\n",
        "        \n",
        "        if is_training:\n",
        "            embeds = self.dropout(embeds)\n",
        "        out, hidden = self.rnn(embeds, hidden)\n",
        "        \n",
        "        return self.linear(out.contiguous().view(out.size(0) * out.size(1), -1)), hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JnT_Wbu07-Ew",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "MKrSfhIk7-E1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "kVnfLhsn7-E5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBED_SIZE = 128\n",
        "HIDDEN_SIZE = 1024\n",
        "NUM_LAYER = 1\n",
        "LR = 0.01\n",
        "SEQ_LENGTH = 30 # for bptt\n",
        "BATCH_SIZE = 100\n",
        "EPOCH = 10\n",
        "VOCAB_SIZE = len(word2index)\n",
        "DROPOUT_PROB = 0.5\n",
        "USE_RESCHEDULING = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H9ZEAz3P7-FE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = batchify(train_data, BATCH_SIZE)\n",
        "dev_data = batchify(dev_data, BATCH_SIZE//2)\n",
        "test_data = batchify(test_data, BATCH_SIZE//2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-KMJtRI7-FR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LanguageModel(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYER, DROPOUT_PROB)\n",
        "model.init_weight() \n",
        "if USE_CUDA:\n",
        "    model = model.cuda()\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8xpoAw47-Fs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rescheduled = False\n",
        "for epoch in range(EPOCH):\n",
        "    total_loss = 0\n",
        "    losses = []\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "    for i,batch in enumerate(get_batch(train_data, SEQ_LENGTH)):\n",
        "        inputs, targets = batch\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        preds, hidden = model(inputs, hidden, True)\n",
        "\n",
        "        loss = loss_function(preds, targets.view(-1))\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        if i > 0 and i % 50 == 0:\n",
        "            print(\"[{0:2d}/{1:2d}] mean_loss : {2:.2f}, Perplexity : {3:.2f}\".format(epoch, EPOCH, np.mean(losses), np.exp(np.mean(losses))))\n",
        "            losses = []\n",
        "      \n",
        "        if USE_RESCHEDULING:\n",
        "          # learning rate anealing\n",
        "          # You can use http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate\n",
        "          if rescheduled == False and epoch == EPOCH//2:\n",
        "              LR *= 0.1\n",
        "              optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "              rescheduled = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "waP4Yz8R7-Gm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ]
    },
    {
      "metadata": {
        "id": "UIPXZLZL7-Gp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_loss = 0\n",
        "hidden = model.init_hidden(BATCH_SIZE//2)\n",
        "for batch in get_batch(test_data, SEQ_LENGTH):\n",
        "    inputs,targets = batch\n",
        "        \n",
        "    hidden = model.detach_hidden(hidden)\n",
        "    model.zero_grad()\n",
        "    preds, hidden = model(inputs, hidden)\n",
        "    total_loss += inputs.size(1) * loss_function(preds, targets.view(-1)).data\n",
        "\n",
        "total_loss = total_loss.item()/test_data.size(1)\n",
        "print(\"Test Perpelexity : {:.2f}\".format(np.exp(total_loss)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mVGUtPLnC1WN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_SAMPLES = 500\n",
        "\n",
        "with torch.no_grad():\n",
        "    with open('sample.txt', 'w') as f:\n",
        "        # Set intial hidden ane cell states\n",
        "        state = (torch.zeros(NUM_LAYER, 1, HIDDEN_SIZE).to(device),\n",
        "                 torch.zeros(NUM_LAYER, 1, HIDDEN_SIZE).to(device))\n",
        "\n",
        "        # Select one word id randomly\n",
        "        prob = torch.ones(VOCAB_SIZE)\n",
        "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "\n",
        "        for i in range(NUM_SAMPLES):\n",
        "            # Forward propagate RNN \n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = index2word[word_id]\n",
        "            word = '\\n' if word in ['<eos>', '</s>'] else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "              print('Sampled [{}/{}] words and saved to {}'.format(i+1, NUM_SAMPLES, 'sample.txt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFNtXYoKFGgp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NUM_SAMPLED_WORDS = 200\n",
        "NUM_SAMPLES = 5\n",
        "\n",
        "def sequence_to_variables(seq, word2index):\n",
        "  return [Variable(x).view(1, 1) for x in prepare_sequence(seq, word2index)]\n",
        "\n",
        "starting_sequence = ['this', 'is', 'the']\n",
        "\n",
        "with torch.no_grad():\n",
        "    with open('sample_prefilled.txt', 'w') as f:\n",
        "      for k in range(NUM_SAMPLES):\n",
        "        # Set intial hidden ane cell states\n",
        "        state = (torch.zeros(NUM_LAYER, 1, HIDDEN_SIZE).to(device),\n",
        "                 torch.zeros(NUM_LAYER, 1, HIDDEN_SIZE).to(device))\n",
        "\n",
        "        seq_of_variables = sequence_to_variables(starting_sequence, word2index)\n",
        "\n",
        "        for input in seq_of_variables[:-1]:          \n",
        "          output, state = model(input, state)\n",
        "\n",
        "        input = seq_of_variables[:1][0]\n",
        "        for i in range(NUM_SAMPLED_WORDS):\n",
        "            # Forward propagate RNN \n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = index2word[word_id]\n",
        "            word = '\\n' if word in ['<eos>', '</s>'] else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "              print('Sampled {} [{}/{}] words and saved to {}'.format(k+1, i+1, NUM_SAMPLED_WORDS, 'sample_prefilled.txt'))\n",
        "              \n",
        "        f.write('\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "sWdQ_KOz7-HP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Further topics"
      ]
    },
    {
      "metadata": {
        "id": "1QlOOiZ07-HS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* <a href=\"https://arxiv.org/pdf/1609.07843.pdf\">Pointer Sentinel Mixture Models</a>\n",
        "* <a href=\"https://arxiv.org/pdf/1708.02182\">Regularizing and Optimizing LSTM Language Models</a>"
      ]
    },
    {
      "metadata": {
        "id": "EFWFG0wR7-Hf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
