{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_fastText.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "gG3Y3QNaklub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Classification with fastText\n",
        "\n",
        "This quick tutorial introduces the task of text classification using the [fastText](https://fasttext.cc/) library and tries to show what the full pipeline looks like from the beginning (obtaining the dataset and preparing the train/valid split) to the end (predicting labels for unseen input data)."
      ]
    },
    {
      "metadata": {
        "id": "_DdFjtLj1qis",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Cooking StackExchange tags dataset\n",
        "\n",
        "We'll use a dataset of a few thousand questions asked on [Cooking StackExchange](https://cooking.stackexchange.com/) which have various tags assigned to them and which already exists in the fastText format -- basically a text file where each line contains one text document that is to be classified. Note that the lines start with `__label__` tags which denote the \"ground truth\" label for that particular text document.\n",
        "\n",
        "\n",
        "`__label__<X> __label__<Y> ... <Text>`\n",
        "\n",
        "\n",
        "For example:\n",
        "\n",
        "`__label__chocolate American equivalent for British chocolate terms`\n",
        "\n",
        "--------------------------\n",
        "\n",
        "In the next few cells we'll download the dataset and take a closer look at what the data looks like (using the [`head`](https://linux.101hacks.com/unix/head/) command) and some further statistics about the dataset (using the [`wc`](https://www.tecmint.com/wc-command-examples/) -- command).\n"
      ]
    },
    {
      "metadata": {
        "id": "UODjsAxt1ono",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz && tar xvzf cooking.stackexchange.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FmlmQB42tfL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head cooking.stackexchange.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyPeaPL03bTw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wc cooking.stackexchange.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8IWotaq83op6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We've got roughly 15k samples in our dataset. Let's split it into a training set of roughly 12k samples and testing set of 3k samples."
      ]
    },
    {
      "metadata": {
        "id": "MR_Yj97c3fD3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head -n 12404 cooking.stackexchange.txt > cooking.train\n",
        "!tail -n 3000 cooking.stackexchange.txt > cooking.valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d4QTcoLc4U1E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation of fastText\n",
        "\n",
        "Installing fastText is realtively easy on any Unix-like system -- running the following cell should be enough to build the `fasttext` binary, which is all we need in this tutorial."
      ]
    },
    {
      "metadata": {
        "id": "wFTFf8_C4SVp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/facebookresearch/fastText/archive/v0.2.0.zip\n",
        "!unzip v0.2.0.zip\n",
        "%cd fastText-0.2.0\n",
        "!make\n",
        "!cp fasttext ../\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMM0RoLK6BBM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and testing a fastText model"
      ]
    },
    {
      "metadata": {
        "id": "uMi85o9H858P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The actual model fastText implements is rather simple as we can see in the image below -- the final log-likelihood the model tries to optimize in training is \n",
        "\n",
        "$$ - \\frac{1}{N} \\sum_{n=1}^{N} y_n \\log(f(BAx_n)) $$\n",
        "\n",
        "where \n",
        "- $x_n$ is the one-hot encoded representation of a word\n",
        "- $A$ is the word embedding matrix\n",
        "- $B$ is the linear projection from word embeddings to output classes\n",
        "- $f$ is the `softmax` non-linearity function\n",
        "\n",
        "You can find more details on the model in the introductory paper: [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759).\n",
        "\n",
        "\n",
        "![The actual model architecture of fastText classification](https://cdn-images-1.medium.com/max/800/1*AgrrRZ9DpUVb3srTWs0gzA.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "cr70hgu38eCY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following cell we run the `supervised` command which trains a fastText model using the data in `./cooking.train` and saves the model to `./cooking_model1`."
      ]
    },
    {
      "metadata": {
        "id": "vhi9ln0x4ues",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!./fasttext supervised -input ./cooking.train -output ./cooking_model1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zjF9yEkc-XOw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's see how the model does on the validation set."
      ]
    },
    {
      "metadata": {
        "id": "-xEv2S4846Ky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!./fasttext test cooking_model1.bin ./cooking.valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2T6yt2RR62rH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looking at the results, they do not look very stellar. The `P@1`metrics represents the [precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) at the first topmost predicted class, while `R@1` represents the [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) at the first topmost predicted class and their respective values leave a lot to be desired.\n",
        "\n",
        "Let's see what options does fastText allow us to set and see if we can get it to perform better"
      ]
    },
    {
      "metadata": {
        "id": "QfKko9217kBR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!./fasttext supervised"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2g1jFzxw__WR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are a couple of interesting options we'll dive a bit deeper into:\n",
        "\n",
        "#### Character ngrams (`minx` and `maxn`)\n",
        "\n",
        "One of the interesting things fastText is capable of doing is incorporating character level information when preparing word vectors. You can find all the glory details in the [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606) paper, but the basic idea is as follows:\n",
        "\n",
        "Given the word `banana` and $n=3$, fastText would generate the following ngrams:\n",
        "\n",
        "- `<ba`\n",
        "- `ban`\n",
        "- `ana`\n",
        "- `nan`\n",
        "- `ana`\n",
        "- `na>`\n",
        "\n",
        "where the `<` and `>` represent the beginning and end of the word, respectively. That is quite useful because if we also had the word *ban* as part of the vocabulary, it would be represented as `<ban>` which makes it distinguishable from `ban` we extracted from banana.\n",
        "\n",
        "Note that we are still talking about bag of words model and thus only the presence of a respective ngram matters. Still, thanks to this nice setup we are pretty much by default able to model prefixes and suffixes. That is of huge practical value, since even if we now encountered say the word `bananoid`  which was not present in training data, thanks to the aforementioned character ngrams we are able to assign it at least some representation, rather than calling it an unknown word and replacing its occurences with `UNK`, which is what the standard approach would be.\n",
        "\n",
        "In fastText the length of ngrams can be set via the `-minn` and `-maxn` flags, which control the minimum and maximum length of ngrams fastText considers. By default these are set to 0, which basically turns this feature off.\n",
        "\n",
        "Let's see if our `bananoid` example would actually work by saving the word vectors fastText produces during training and trying to find out which words are the closest neighbors of `bananoid` in the learned vector space"
      ]
    },
    {
      "metadata": {
        "id": "UlkFo6EpeDKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!./fasttext supervised -minn 3 -maxn 5 -input ./cooking.train -output ./cooking_model1 -saveOutput 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fw6DBJi4eGhc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!echo \"bananoid\" | ./fasttext nn ./cooking_model1.bin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LFrjqHMZdPhi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Word ngrams\n",
        "\n",
        "Similarly to character ngrams, fastText can also generate ngrams from words in the document. This can be set using the `-wordNgrams` flag which is set to 1 by default: only unigrams (single words) are considered. When we set it to say 2, the sentece `smash all potatoes` would be represented as\n",
        "\n",
        "- `<smash>`\n",
        "- `<all>`\n",
        "- `<potatoes>`\n",
        "- `<smash all>`\n",
        "- `<all potatoes`\n",
        "\n",
        "-----------------\n",
        "\n",
        "Using these and some of the other available options, let us train a new version of the model and see how it performs.\n"
      ]
    },
    {
      "metadata": {
        "id": "wFF_zi5a6P9_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!./fasttext supervised -minCount 2 -wordNgrams 3 -minn 3 -maxn 8 -lr 0.7 -dim 100 -epoch 25 -input ./cooking.train -output cooking_model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1bSihncU6dm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!./fasttext test cooking_model2.bin ./cooking.valid 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1irrfPU8gNO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks a bit better, right?\n",
        "\n",
        "Note that the command above outputs precision/recall for just the top 1 example. In many cases, however, we maybe more interested in knowing whether the \"true\" labels could be found in the say top 5 predictions, especially since many of them have more than one tag assigned.\n",
        "\n",
        "We can easily compute precision/recall in this way by executing"
      ]
    },
    {
      "metadata": {
        "id": "ckiriz80_wSa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!./fasttext test cooking_model2.bin ./cooking.valid 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bzAvHmafXvd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looking at just the summary statistics is not really that much fun -- that usually comes from trying the model out on some real-world data. We can easily do that with fastText by running something like the command in the following cell:"
      ]
    },
    {
      "metadata": {
        "id": "godG5jDhf0tE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!echo \"Does it make sense to cook smashed potatoes?\" | ./fasttext predict-prob ./cooking_model2.bin -"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2gIhCAvzgK3r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Alternatively we can also ask for more than just the most probable label:"
      ]
    },
    {
      "metadata": {
        "id": "-yl7hOlqgRLE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!echo \"Does it make sense to cook smashed potatoes?\" | ./fasttext predict-prob ./cooking_model2.bin - 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I2j07FbWgh0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Or ask for as many predictions as possible (`-1`) but only taking into account those that have probability higher than `0.02`:"
      ]
    },
    {
      "metadata": {
        "id": "V0NdM7SqgexL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!echo \"Does it make sense to cook smashed potatoes?\" | ./fasttext predict-prob ./cooking_model2.bin - -1 0.02"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V6GEl-hx_Hw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Your tasks\n",
        "\n",
        "1. See if you can improve the model further -- try to optimize both Precision and Recall at 3 predictions\n",
        "2. Try to see if some pre-processing (lowercasing, removing stop words, punctuation, ...) would be helpful here (the `bananoid` example does really suggest so). Note that fastText splits tokens on whitespace it finds in the input data, so it is not uncommon to find out that it learned word vectors for words like `banana?` among others. If you are looking for an industry-grade tokenizer, I strongly recommend [BlingFire](https://github.com/Microsoft/BlingFire). \n",
        "3. See if you can use some of the same ideas on a different [Amazon Sentiment Analysis dataset](https://storage.googleapis.com/amazonreviews/train.ft.txt.bz2) and get the testing precision/recall over 0.9!\n",
        "\n",
        "**Bonus**: the choice of checking both Precision and Recall at 3 or 5 is rather arbitrary. Analyze the training data and find out what number would really make sense, based on the number of labels the considered documents usually have.\n",
        "\n",
        "For a more in-depth walkthrough of fastText's internals please reffer to [FastText: Under the Hood](https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3)"
      ]
    }
  ]
}